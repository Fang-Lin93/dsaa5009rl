{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Reinforcement learning through Self-play for Super-Tic-Tac-Toe\n",
    "\n",
    "This notebook aims to give a simple tutorial of the running logic of this project. Here is the outline:\n",
    "   1. Super-Tic-Tac-Toe environment\n",
    "   2. Double DQN agent\n",
    "   3. Replay buffer\n",
    "   4. Training/Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Super-Tic-Tac-Toe environment\n",
    "\n",
    "The environment [Super-Tic-Tac-Toe](envs/super_tic_tac_toe.py) extends the original Tic-Tac-Toe environment by introducing two features to increase the difficulty: larger board (6x6) and stochastic transitions. For simplicity, we only consider the larger board in this tutorial.\n",
    "\n",
    "Mimicking how human visually read a board state, I represent the board state as a 6x6x2 0-1 tensor with 1 indicating occupied positions and 0 empty cells, respectively. Here are two channels for each cell with each corresponding to one specific player. Here is how to initialize an environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:32:41.692704Z",
     "start_time": "2024-03-25T07:32:38.456276Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Aug 15 2022 11:36:51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "board shape = (6, 6, 2)\n",
      "legal_actions mask shape = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "current player_id = 1\n",
      "The initial board:\n",
      "| [0] [1] [2] [3] [4] [5]|\n",
      "| [6] [7] [8] [9][10][11]|\n",
      "|[12][13][14][15][16][17]|\n",
      "|[18][19][20][21][22][23]|\n",
      "|[24][25][26][27][28][29]|\n",
      "|[30][31][32][33][34][35]|\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from envs import SuperTicTacToeEnv\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "env = SuperTicTacToeEnv(random_place_prob=0.)  # no stochastic transitions.\n",
    "\n",
    "board, la_mask, player_id, done = env.reset()\n",
    "        \n",
    "print('board shape =', board.shape)\n",
    "print('legal_actions mask shape =', la_mask)  # 1 indicates available actions, explained later\n",
    "print('current player_id =', player_id)\n",
    "\n",
    "print('The initial board:')\n",
    "env.render(show_act_idx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each environment step receives an action and returns several useful information: `board state`, `legal action mask`, `current player id`, `done signal` and the `rewards`. The accepted action is one of the 36 integers ranging from 0 to 35, indicating the position index.\n",
    "\n",
    "The default setting for reward value is: +10 for winning, 0 otherwise.\n",
    "You can try different reward designs, such as (win, lose, otherwise) = (+1, -1, 0). You need to modify the codes in the `env.step` method.\n",
    "\n",
    "The initial player is randomly chosen for fairness. Here I finish the game with randomly chosen actions. You can visually check the board using `env.render()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:32:41.693323Z",
     "start_time": "2024-03-25T07:32:41.669034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winner = 0\n",
      "rewards = [10, 0]\n",
      "Visualizing the env:\n",
      "|  X   O   O   X   O   X |\n",
      "|  O   X   O   O   O   X |\n",
      "|  X   X [14]  O   X [17]|\n",
      "|  O   O   O   X   O   X |\n",
      "|  O   X   O   O   X   X |\n",
      "|  X   O   X   X   X   O |\n"
     ]
    }
   ],
   "source": [
    "# finish the game with random actions\n",
    "while not done:\n",
    "    a = np.random.choice(env.get_legal_actions())\n",
    "    (board, la_mask, player_id, done), rewards = env.step(a)\n",
    "\n",
    "print('winner =', env.winner)\n",
    "print('rewards =', rewards)\n",
    "print('Visualizing the env:')\n",
    "env.render(show_act_idx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **54** winner patterns. Here I list 3 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:32:41.693681Z",
     "start_time": "2024-03-25T07:32:41.675596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of winner patterns = 54\n",
      "|  O   O   O   O   _   _ |\n",
      "|  _   _   _   _   _   _ |\n",
      "|  _   _   _   _   _   _ |\n",
      "|  _   _   _   _   _   _ |\n",
      "|  _   _   _   _   _   _ |\n",
      "|  _   _   _   _   _   _ |\n",
      "\n",
      "|  _   _   _   _   _   _ |\n",
      "|  _   _   _   O   _   _ |\n",
      "|  _   _   _   O   _   _ |\n",
      "|  _   _   _   O   _   _ |\n",
      "|  _   _   _   O   _   _ |\n",
      "|  _   _   _   _   _   _ |\n",
      "\n",
      "|  _   _   _   _   _   _ |\n",
      "|  _   _   _   _   _   _ |\n",
      "|  _   O   _   _   _   _ |\n",
      "|  _   _   O   _   _   _ |\n",
      "|  _   _   _   O   _   _ |\n",
      "|  _   _   _   _   O   _ |\n"
     ]
    }
   ],
   "source": [
    "winner_patterns = env.get_winner_pattern()\n",
    "\n",
    "print('# of winner patterns =', len(winner_patterns))\n",
    "env.render(winner_patterns[0], show_act_idx=False)  # row example\n",
    "env.render(winner_patterns[21], show_act_idx=False)  # column example\n",
    "env.render(winner_patterns[50], show_act_idx=False)  # diagonal example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations (reinforcement learning states)\n",
    "\n",
    "The Super-Tic-Tac-Toe is a perfect information game, where the board state is fully observable for both players, i.e., the *observations* are the same as the environment *states*. However, the `env.board` is not the observation used by the agent, as the agent should also know its player_id as well.\n",
    " \n",
    "To simply the representation of the observations for the neural network agent to learn, I define the observations as **6x6x2** images with a little re-arrangement of the `env.board`: with the first channel **always** representing 'my positions' and the second channel **always** encoding the 'opponent positions'. To get the observations, I need to fetch the board state using the player_id:\n",
    " \n",
    "`observation = board[:, :, [player_id, 1-player_id]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:32:41.694006Z",
     "start_time": "2024-03-25T07:32:41.679766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my player_id =  1\n",
      "my observations ('O' for myself and 'X' for the opponent):\n",
      "|  O   X   X   O   X   O |\n",
      "|  X   O   X   X   X   O |\n",
      "|  O   O [14]  X   O [17]|\n",
      "|  X   X   X   O   X   O |\n",
      "|  X   O   X   X   O   O |\n",
      "|  O   X   O   O   O   X |\n",
      "\n",
      "my positions:\n",
      "|  X   _   _   X   _   X |\n",
      "|  _   X   _   _   _   X |\n",
      "|  X   X   _   _   X   _ |\n",
      "|  _   _   _   X   _   X |\n",
      "|  _   X   _   _   X   X |\n",
      "|  X   _   X   X   X   _ |\n",
      "\n",
      "opponent positions:\n",
      "|  _   O   O   _   O   _ |\n",
      "|  O   _   O   O   O   _ |\n",
      "|  _   _   _   O   _   _ |\n",
      "|  O   O   O   _   O   _ |\n",
      "|  O   _   O   O   _   _ |\n",
      "|  _   O   _   _   _   O |\n"
     ]
    }
   ],
   "source": [
    "my_obs = board[:, :, [player_id, 1-player_id]]\n",
    "\n",
    "print('my player_id = ', player_id)\n",
    "print(\"my observations ('O' for myself and 'X' for the opponent):\")\n",
    "env.render(my_obs, show_act_idx=True)\n",
    "\n",
    "print('my positions:')\n",
    "env.render(my_obs[:, :, 0], mark=player_id, show_act_idx=False)\n",
    "\n",
    "print('opponent positions:')\n",
    "env.render(my_obs[:, :, 1], mark=1 - player_id, show_act_idx=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "Each action is an integer ranging from 0 to 35, indicating the index of the 6x6 board cells. The action should be chosen from the legal action sets (empty positions). The environment provides 2 ways for choosing legal action: masks and sets. \n",
    "\n",
    "* The `legal action masks` *m* are convenient for downstream probabilistic polices. For example, if the predicted policy distribution is *p* (36-dim vector), then the action can be sampled from *p Hadamard_product m*, where the occupied positions always have zero sampling probability.\n",
    "\n",
    "* The `legal action set` is convenient for some other methods, such as Monte-Carlo Tree Search.\n",
    "\n",
    "You can choose your own preferences for legal action representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:32:41.694319Z",
     "start_time": "2024-03-25T07:32:41.682890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legal action masks from env.step: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "legal action set: [14, 17]\n"
     ]
    }
   ],
   "source": [
    "# 1. legal action masks: 1 for available positions; 0 for occupied positions\n",
    "print('legal action masks from env.step:', la_mask)\n",
    "\n",
    "# 2. index set of legal actions\n",
    "print('legal action set:', env.get_legal_actions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Double DQN agent\n",
    "\n",
    "For simplicity, I define the [base agent framework](agents/agent.py) in my code. Each agent should define its own `update(self, batch: Batch)` method for how to train from a data batch and the `sample_actions(self, *args, **kwargs)` method for how to sample actions. \n",
    "\n",
    "The Double DQN agent requires two Q-networks: a policy Q-network and a target Q-network. The policy Q-network is trained directly through the gradient descent. The target Q-network is the exponential moving average of learned policy Q-network.\n",
    "\n",
    "As the board observations is much like images with 2 channels, I define the Q-network as a 2D convolutional neural network for the game:\n",
    "\n",
    "* Layers: 3 hidden layers with each of the dimensions equaling 64, the final activation of CNN layers squeezes the dimension to 1.\n",
    "* Activation: [mish](https://arxiv.org/abs/1908.08681).\n",
    "* Final activation layer: a MLP layer to project the activation of the CNN to the action dimension by `nn.Dense(out_dim)` in Jax or `nn.Linear(hidden_dims[-1] * height * width, out_dims)` in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of the Q-network module represented as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:32:41.956029Z",
     "start_time": "2024-03-25T07:32:41.687046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[3m                                  CNN Summary                                   \u001B[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
      "┃\u001B[1m \u001B[0m\u001B[1mpath   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mmodule\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1minputs    \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1moutputs  \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mflops  \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mvjp_flops\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mparams    \u001B[0m\u001B[1m \u001B[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
      "│         │ CNN    │ \u001B[2mfloat32\u001B[0m[1… │ \u001B[2mfloat32\u001B[0m[… │ 4364395 │ 13128384  │            │\n",
      "├─────────┼────────┼────────────┼───────────┼─────────┼───────────┼────────────┤\n",
      "│ Conv_0  │ Conv   │ \u001B[2mfloat32\u001B[0m[1… │ \u001B[2mfloat32\u001B[0m[… │ 67840   │ 202368    │ bias:      │\n",
      "│         │        │            │           │         │           │ \u001B[2mfloat32\u001B[0m[6… │\n",
      "│         │        │            │           │         │           │ kernel:    │\n",
      "│         │        │            │           │         │           │ \u001B[2mfloat32\u001B[0m[3… │\n",
      "│         │        │            │           │         │           │            │\n",
      "│         │        │            │           │         │           │ \u001B[1m1,216 \u001B[0m\u001B[1;2m(4.9\u001B[0m │\n",
      "│         │        │            │           │         │           │ \u001B[1;2mKB)\u001B[0m        │\n",
      "├─────────┼────────┼────────────┼───────────┼─────────┼───────────┼────────────┤\n",
      "│ Conv_1  │ Conv   │ \u001B[2mfloat32\u001B[0m[1… │ \u001B[2mfloat32\u001B[0m[… │ 2099456 │ 6332928   │ bias:      │\n",
      "│         │        │            │           │         │           │ \u001B[2mfloat32\u001B[0m[6… │\n",
      "│         │        │            │           │         │           │ kernel:    │\n",
      "│         │        │            │           │         │           │ \u001B[2mfloat32\u001B[0m[3… │\n",
      "│         │        │            │           │         │           │            │\n",
      "│         │        │            │           │         │           │ \u001B[1m36,928 \u001B[0m    │\n",
      "│         │        │            │           │         │           │ \u001B[1;2m(147.7 KB)\u001B[0m │\n",
      "├─────────┼────────┼────────────┼───────────┼─────────┼───────────┼────────────┤\n",
      "│ Conv_2  │ Conv   │ \u001B[2mfloat32\u001B[0m[1… │ \u001B[2mfloat32\u001B[0m[… │ 2099456 │ 6332928   │ bias:      │\n",
      "│         │        │            │           │         │           │ \u001B[2mfloat32\u001B[0m[6… │\n",
      "│         │        │            │           │         │           │ kernel:    │\n",
      "│         │        │            │           │         │           │ \u001B[2mfloat32\u001B[0m[3… │\n",
      "│         │        │            │           │         │           │            │\n",
      "│         │        │            │           │         │           │ \u001B[1m36,928 \u001B[0m    │\n",
      "│         │        │            │           │         │           │ \u001B[1;2m(147.7 KB)\u001B[0m │\n",
      "├─────────┼────────┼────────────┼───────────┼─────────┼───────────┼────────────┤\n",
      "│ Conv_3  │ Conv   │ \u001B[2mfloat32\u001B[0m[1… │ \u001B[2mfloat32\u001B[0m[… │ 32804   │ 98952     │ bias:      │\n",
      "│         │        │            │           │         │           │ \u001B[2mfloat32\u001B[0m[1] │\n",
      "│         │        │            │           │         │           │ kernel:    │\n",
      "│         │        │            │           │         │           │ \u001B[2mfloat32\u001B[0m[3… │\n",
      "│         │        │            │           │         │           │            │\n",
      "│         │        │            │           │         │           │ \u001B[1m577 \u001B[0m\u001B[1;2m(2.3 \u001B[0m  │\n",
      "│         │        │            │           │         │           │ \u001B[1;2mKB)\u001B[0m        │\n",
      "├─────────┼────────┼────────────┼───────────┼─────────┼───────────┼────────────┤\n",
      "│ Dense_0 │ Dense  │ \u001B[2mfloat32\u001B[0m[1… │ \u001B[2mfloat32\u001B[0m[… │ 2628    │ 9144      │ bias:      │\n",
      "│         │        │            │           │         │           │ \u001B[2mfloat32\u001B[0m[3… │\n",
      "│         │        │            │           │         │           │ kernel:    │\n",
      "│         │        │            │           │         │           │ \u001B[2mfloat32\u001B[0m[3… │\n",
      "│         │        │            │           │         │           │            │\n",
      "│         │        │            │           │         │           │ \u001B[1m1,332 \u001B[0m\u001B[1;2m(5.3\u001B[0m │\n",
      "│         │        │            │           │         │           │ \u001B[1;2mKB)\u001B[0m        │\n",
      "├─────────┼────────┼────────────┼───────────┼─────────┼───────────┼────────────┤\n",
      "│\u001B[1m \u001B[0m\u001B[1m       \u001B[0m\u001B[1m \u001B[0m│\u001B[1m \u001B[0m\u001B[1m      \u001B[0m\u001B[1m \u001B[0m│\u001B[1m \u001B[0m\u001B[1m          \u001B[0m\u001B[1m \u001B[0m│\u001B[1m \u001B[0m\u001B[1m         \u001B[0m\u001B[1m \u001B[0m│\u001B[1m \u001B[0m\u001B[1m       \u001B[0m\u001B[1m \u001B[0m│\u001B[1m \u001B[0m\u001B[1m    Total\u001B[0m\u001B[1m \u001B[0m│\u001B[1m \u001B[0m\u001B[1m76,981    \u001B[0m\u001B[1m \u001B[0m│\n",
      "│\u001B[1m         \u001B[0m│\u001B[1m        \u001B[0m│\u001B[1m            \u001B[0m│\u001B[1m           \u001B[0m│\u001B[1m         \u001B[0m│\u001B[1m           \u001B[0m│\u001B[1m \u001B[0m\u001B[1;2m(307.9 KB)\u001B[0m\u001B[1m \u001B[0m│\n",
      "└─────────┴────────┴────────────┴───────────┴─────────┴───────────┴────────────┘\n",
      "\u001B[1m                                                                                \u001B[0m\n",
      "\u001B[1m                      Total Parameters: 76,981 \u001B[0m\u001B[1;2m(307.9 KB)\u001B[0m\u001B[1m                       \u001B[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from flax import linen as nn\n",
    "from networks.nets import CNN, mish\n",
    "\n",
    "critic_def = CNN(hidden_dims=(64, 64, 64, 1),\n",
    "                 out_dim=36,  # number of action dimensions = 36\n",
    "                 activations=mish,  # A Self Regularized Non-Monotonic Activation Function\n",
    "                 dropout_rate=0.,\n",
    "                 layer_norm=False)\n",
    "\n",
    "print(nn.tabulate(critic_def, jax.random.key(0), compute_flops=True, compute_vjp_flops=True)(jax.numpy.zeros((1,6,6,2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packaged Double DQN agent is given in the [jax_dqn.py](agents/jax_dqn.py) or [torch_dqn.py](agents/torch_dqn.py) script. In my tutorial, I follow some conventional settings to create the agent:\n",
    "\n",
    "* Discount rate: 0.9\n",
    "* Epsilon greedy: starting from 1 and linearly decay to 0.1\n",
    "* Gradient descent: [Adam](https://arxiv.org/abs/1412.6980) with cosine decay learning rate, starting from 0.0003\n",
    "* Updating target Q-network: exponential moving average with learning rate 0.005\n",
    "\n",
    "Here is an example of defining the agent: `max_grad_steps` defines the total number of gradient steps. It may take a much larger value (> 10 million) to converge!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:32:42.146144Z",
     "start_time": "2024-03-25T07:32:41.952315Z"
    }
   },
   "outputs": [],
   "source": [
    "from agents.agent import Agent\n",
    "# from agents.jax_dqn import DQNLearner  # Jax example\n",
    "from agents.torch_dqn import DQNLearner  # Torch example\n",
    "\n",
    "max_grad_steps = 10000\n",
    "agent = DQNLearner(seed=0,\n",
    "                   hidden_dims=(64, 64, 64, 1),\n",
    "                   obs_shape=(6, 6, 2),\n",
    "                   act_dim=36,\n",
    "                   lr_decay_steps=max_grad_steps,\n",
    "                   greedy_max=1,  # eps-greedy \n",
    "                   greedy_min=0.1,\n",
    "                   discount=0.9,\n",
    "                   greedy_decay_steps=50,\n",
    "                   ema_tau=0.005)\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    name = 'random_agent'\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sample_actions(self, observations, legal_actions) -> int:\n",
    "        return np.random.choice(legal_actions.reshape(-1).nonzero()[0])\n",
    "\n",
    "    def update_state(self, state: dict) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training process of Double DQN, the Bellman target is computed requiring both the policy Q-network and the target Q-network. The policy Q-network gives the greedy actions while the target Q-network evaluate the greedy actions to alleviate the overestimation. See the [paper](https://arxiv.org/abs/1509.06461v3) or my [codes](agents/torch_dqn.py) for the details of the implementation of Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Replay buffer\n",
    "\n",
    "The replay buffer stores the transitions for the Q-learning. Each transition contains a tuple: \n",
    "\n",
    "``\n",
    "(observation, action, reward, next_observation, mask).\n",
    "``\n",
    "\n",
    "The `mask = 1 - done` is for computing the Bellman target `target_q = reward + discount * masks * next_q`. \n",
    "\n",
    "In this game, the board has rotation symmetries: a board rotated by `n*90` degrees actually gives the same observation. To increase the sampling efficiency, we can expand the transition data by rotating the board observation by `n*90 (n=1, 2, 3)` degrees and store them into the buffer as well. \n",
    "\n",
    "In fact, the reflection (transpose) transformation also gives the same observations. Furthermore, each transformation with the minimal rotation granularity = 90-degree inside the orthogonal group O(6) also gives the same observations. For simplicity, here I only consider the rotations. You can try other symmetry transforms as you like.\n",
    "\n",
    "Here are examples of rotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:32:42.153855Z",
     "start_time": "2024-03-25T07:32:42.147372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my observations:\n",
      "|  O   X   X   O   X   O |\n",
      "|  X   O   X   X   X   O |\n",
      "|  O   O   _   X   O   _ |\n",
      "|  X   X   X   O   X   O |\n",
      "|  X   O   X   X   O   O |\n",
      "|  O   X   O   O   O   X |\n",
      "\n",
      "rotated 90 degrees:\n",
      "|  O   O   _   O   O   X |\n",
      "|  X   X   O   X   O   O |\n",
      "|  O   X   X   O   X   O |\n",
      "|  X   X   _   X   X   O |\n",
      "|  X   O   O   X   O   X |\n",
      "|  O   X   O   X   X   O |\n",
      "\n",
      "rotated 180 degrees:\n",
      "|  X   O   O   O   X   O |\n",
      "|  O   O   X   X   O   X |\n",
      "|  O   X   O   X   X   X |\n",
      "|  _   O   X   _   O   O |\n",
      "|  O   X   X   X   O   X |\n",
      "|  O   X   O   X   X   O |\n",
      "\n",
      "rotated 270 degrees:\n",
      "|  O   X   X   O   X   O |\n",
      "|  X   O   X   O   O   X |\n",
      "|  O   X   X   _   X   X |\n",
      "|  O   X   O   X   X   O |\n",
      "|  O   O   X   O   X   X |\n",
      "|  X   O   O   _   O   O |\n"
     ]
    }
   ],
   "source": [
    "print('my observations:')\n",
    "env.render(my_obs)\n",
    "print('rotated 90 degrees:')\n",
    "env.render(np.rot90(my_obs))\n",
    "print('rotated 180 degrees:')\n",
    "env.render(np.rot90(np.rot90(my_obs)))\n",
    "print('rotated 270 degrees:')\n",
    "env.render(np.rot90(np.rot90(np.rot90(my_obs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding actions should also be carefully transferred to match the rotated broads. The transformation is given as the following:\n",
    "\n",
    "``\n",
    "row, col = action // 6, action % 6\n",
    "``\n",
    "``\n",
    "rotated_action = 6 * (5 - col) + row\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of action rotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:32:42.154252Z",
     "start_time": "2024-03-25T07:32:42.151089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action (marked as 'O') is rotated 90 degrees to a new position (marked as 'X'): \n",
      "|  _   _   _   _   _   _ |\n",
      "|  _   _   X   _   _   _ |\n",
      "|  _   _   _   _   O   _ |\n",
      "|  _   _   _   _   _   _ |\n",
      "|  _   _   _   _   _   _ |\n",
      "|  _   _   _   _   _   _ |\n"
     ]
    }
   ],
   "source": [
    "action = 16  # just an example\n",
    "\n",
    "# rotate 90 degrees\n",
    "row, col = action // 6, action % 6\n",
    "rotated_action = 6 * (5 - col) + row\n",
    "\n",
    "# visualization\n",
    "action_board = np.zeros((6, 6, 2))\n",
    "action_board[:, :, 0].reshape(-1)[action] = 1\n",
    "action_board[:, :, 1].reshape(-1)[rotated_action] = 1\n",
    "print(\"Action (marked as 'O') is rotated 90 degrees to a new position (marked as 'X'): \")\n",
    "env.render(action_board)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it is ready to define a replay buffer with capacity=1000000 with rotation expansions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:32:42.162140Z",
     "start_time": "2024-03-25T07:32:42.154508Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets.replay_buffer import ReplayBuffer\n",
    "\n",
    "replay_buffer = ReplayBuffer(obs_shape=(6, 6, 2),\n",
    "                             capacity=1000000,\n",
    "                             rotation_expand_k=3)  # rotated at most 3x90 degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training/Evaluation\n",
    "\n",
    "Up to now, we are prepared with the environment to interact with, the agent ready to learn and the replay buffer to store the online transition data. It is time to define the training and the evaluation process. In this tutorial, I use the random agent as the evaluation opponents. It can be replaced by any pre-trained stronger agents.\n",
    "\n",
    "Let's re-define the environment, the agents and the replay buffer here for completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:32:42.570901Z",
     "start_time": "2024-03-25T07:32:42.158898Z"
    }
   },
   "outputs": [],
   "source": [
    "# some additional imports\n",
    "from matplotlib import pyplot as plt\n",
    "from envs.super_tic_tac_toe import evaluate_tic_tac_toe\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "In my script, I use the tensorboard for statistics tracking. \n",
    "Here I just plot the curves for the tutorial purpose\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "max_steps = 100000  # the max number of gradient steps\n",
    "update_interval = 100  # the frequency for gradient descent\n",
    "eval_interval = 1000  # the frequency for evaluation\n",
    "num_eval = 100  # the number of evaluation repetition\n",
    "log_interval = 500  # the frequency for logging training information\n",
    "buffer_size = 1000000\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "# Environments\n",
    "env = SuperTicTacToeEnv(random_place_prob=0.)  # the main environment to interaction with\n",
    "eval_env = SuperTicTacToeEnv(random_place_prob=0.)  # for evaluation only\n",
    "\n",
    "# Agents\n",
    "agent = DQNLearner(seed=0,\n",
    "                   hidden_dims=(64, 64, 64, 1),\n",
    "                   obs_shape=(6, 6, 2),\n",
    "                   act_dim=36,\n",
    "                   lr_decay_steps=max_steps // update_interval,\n",
    "                   greedy_max=1,\n",
    "                   greedy_min=0.1,\n",
    "                   discount=0.9,\n",
    "                   greedy_decay_steps=max_steps // update_interval)\n",
    "ts_agent = RandomAgent()  # for evaluation only, as the opponent agent\n",
    "\n",
    "# replay buffer\n",
    "replay_buffer = ReplayBuffer(obs_shape=(6, 6, 2),\n",
    "                             capacity=buffer_size,\n",
    "                             rotation_expand_k=3) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the main training/evaluation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:37:34.456521Z",
     "start_time": "2024-03-25T07:32:42.577352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Agent=torch_dqn: 100%|\u001B[32m█████████▉\u001B[0m| 99901/100000 [04:50<00:00, 336.21it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_winning_rate, mean_return, eval_t = [], [], []\n",
    "training_stat, training_t = {}, []\n",
    "\n",
    "# process bar\n",
    "num_steps = 0\n",
    "bar = tqdm(total=max_steps,\n",
    "           desc=f'Train Agent={agent.name}',\n",
    "           smoothing=0.01,\n",
    "           colour='GREEN')\n",
    "\n",
    " # main train/test process\n",
    "while num_steps < max_steps:\n",
    "    board, la, player_id, done = env.reset()\n",
    "\n",
    "    # record transitions for saving to the replay buffer\n",
    "    obs = [None, None]\n",
    "    actions = [None, None]\n",
    "    next_obs = [None, None]\n",
    "    obs[player_id] = board[:, :, [player_id, 1 - player_id]]  # record\n",
    "\n",
    "    # each game process\n",
    "    while not done:\n",
    "        action = agent.sample_actions(observations=board[:, :, [player_id, 1 - player_id]],\n",
    "                                      legal_actions=la)\n",
    "\n",
    "        actions[player_id] = action\n",
    "\n",
    "        (board, la, player_id, done), rewards = env.step(action)\n",
    "        next_obs[player_id] = board[:, :, [player_id, 1 - player_id]]\n",
    "\n",
    "        if done:\n",
    "            # add opponent's transition\n",
    "            replay_buffer.insert(observation=obs[player_id],\n",
    "                                 action=actions[player_id],\n",
    "                                 reward=rewards[player_id],\n",
    "                                 next_observation=board[:, :, [player_id, 1 - player_id]],\n",
    "                                 mask=0)\n",
    "\n",
    "            # add final hand's transition\n",
    "            replay_buffer.insert(observation=obs[1 - player_id],\n",
    "                                 action=actions[1 - player_id],\n",
    "                                 reward=rewards[1 - player_id],\n",
    "                                 next_observation=board[:, :, [1 - player_id, player_id]],\n",
    "                                 mask=0)\n",
    "\n",
    "        else:\n",
    "            if obs[player_id] is not None and next_obs[player_id] is not None:\n",
    "                replay_buffer.insert(observation=obs[player_id],\n",
    "                                     action=actions[player_id],\n",
    "                                     reward=rewards[player_id],\n",
    "                                     next_observation=next_obs[player_id],\n",
    "                                     mask=1)  # continue the game\n",
    "\n",
    "            obs[player_id] = next_obs[player_id]\n",
    "\n",
    "        num_steps += 1\n",
    "        if num_steps < max_steps:\n",
    "            bar.update(1)\n",
    "\n",
    "        if num_steps % update_interval == 0:\n",
    "            batch = replay_buffer.sample(batch_size=batch_size)\n",
    "            if batch:\n",
    "                update_info = agent.update(batch)\n",
    "                # record the training information\n",
    "                if num_steps % log_interval == 0:\n",
    "                    for k, v in update_info.items():\n",
    "                        if k not in training_stat:\n",
    "                            training_stat[k] = []\n",
    "                        training_stat[k].append(v)\n",
    "                    training_t.append(num_steps)\n",
    "                            \n",
    "        if num_steps % eval_interval == 0:\n",
    "            agent.training = False\n",
    "            # test against the opponent agent\n",
    "            wins, returns, _, lens = evaluate_tic_tac_toe([agent, ts_agent], eval_env, num_episodes=num_eval)\n",
    "             \n",
    "            avg_winning_rate.append(wins.count(0)/len(wins))\n",
    "            mean_return.append(returns.mean(axis=0)[0])\n",
    "            eval_t.append(num_steps)\n",
    "            agent.training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:37:34.465147Z",
     "start_time": "2024-03-25T07:37:34.459116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning rate after training: 0.94\n",
      "Mean Return after training: 9.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Winning rate after training: {avg_winning_rate[-1]}\")\n",
    "print(f\"Mean Return after training: {mean_return[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:37:34.601049Z",
     "start_time": "2024-03-25T07:37:34.462222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(eval_t, avg_winning_rate, label='winning rate')\n",
    "plt.legend()\n",
    "plt.title('Average winning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T07:37:36.244003Z",
     "start_time": "2024-03-25T07:37:34.605499Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m training_stat\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     plt\u001B[38;5;241m.\u001B[39mlegend()\n\u001B[1;32m      4\u001B[0m     plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/offlineRL/lib/python3.9/site-packages/matplotlib/pyplot.py:2757\u001B[0m, in \u001B[0;36mplot\u001B[0;34m(scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2755\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mplot)\n\u001B[1;32m   2756\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot\u001B[39m(\u001B[38;5;241m*\u001B[39margs, scalex\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, scaley\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m-> 2757\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgca\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2758\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscalex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscalex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscaley\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscaley\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2759\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m}\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/offlineRL/lib/python3.9/site-packages/matplotlib/axes/_axes.py:1632\u001B[0m, in \u001B[0;36mAxes.plot\u001B[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1390\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1391\u001B[0m \u001B[38;5;124;03mPlot y versus x as lines and/or markers.\u001B[39;00m\n\u001B[1;32m   1392\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1629\u001B[0m \u001B[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001B[39;00m\n\u001B[1;32m   1630\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1631\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m cbook\u001B[38;5;241m.\u001B[39mnormalize_kwargs(kwargs, mlines\u001B[38;5;241m.\u001B[39mLine2D)\n\u001B[0;32m-> 1632\u001B[0m lines \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_lines(\u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39mdata, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)]\n\u001B[1;32m   1633\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines:\n\u001B[1;32m   1634\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_line(line)\n",
      "File \u001B[0;32m~/miniforge3/envs/offlineRL/lib/python3.9/site-packages/matplotlib/axes/_base.py:312\u001B[0m, in \u001B[0;36m_process_plot_var_args.__call__\u001B[0;34m(self, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m    310\u001B[0m     this \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    311\u001B[0m     args \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m--> 312\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plot_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/offlineRL/lib/python3.9/site-packages/matplotlib/axes/_base.py:488\u001B[0m, in \u001B[0;36m_process_plot_var_args._plot_args\u001B[0;34m(self, tup, kwargs, return_kwargs)\u001B[0m\n\u001B[1;32m    486\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(xy) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    487\u001B[0m     x \u001B[38;5;241m=\u001B[39m _check_1d(xy[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m--> 488\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43m_check_1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxy\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    490\u001B[0m     x, y \u001B[38;5;241m=\u001B[39m index_of(xy[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/miniforge3/envs/offlineRL/lib/python3.9/site-packages/matplotlib/cbook/__init__.py:1304\u001B[0m, in \u001B[0;36m_check_1d\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Convert scalars to 1D arrays; pass-through arrays as is.\"\"\"\u001B[39;00m\n\u001B[1;32m   1303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m-> 1304\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43matleast_1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1305\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1306\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1307\u001B[0m         \u001B[38;5;66;03m# work around\u001B[39;00m\n\u001B[1;32m   1308\u001B[0m         \u001B[38;5;66;03m# https://github.com/pandas-dev/pandas/issues/27775 which\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1319\u001B[0m         \u001B[38;5;66;03m# This code should correctly identify and coerce to a\u001B[39;00m\n\u001B[1;32m   1320\u001B[0m         \u001B[38;5;66;03m# numpy array all pandas versions.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/offlineRL/lib/python3.9/site-packages/numpy/core/shape_base.py:65\u001B[0m, in \u001B[0;36matleast_1d\u001B[0;34m(*arys)\u001B[0m\n\u001B[1;32m     63\u001B[0m res \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ary \u001B[38;5;129;01min\u001B[39;00m arys:\n\u001B[0;32m---> 65\u001B[0m     ary \u001B[38;5;241m=\u001B[39m \u001B[43masanyarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mary\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ary\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     67\u001B[0m         result \u001B[38;5;241m=\u001B[39m ary\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/offlineRL/lib/python3.9/site-packages/torch/_tensor.py:1030\u001B[0m, in \u001B[0;36mTensor.__array__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m   1028\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(Tensor\u001B[38;5;241m.\u001B[39m__array__, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1030\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1031\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1032\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k, v in training_stat.items():\n",
    "    plt.plot(training_t, v, label=k)\n",
    "    plt.legend()\n",
    "    plt.title(f'Training {k}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the training process is far from convergence (the performance is still increasing, the q-values are not stable). Here I use a small `max_steps` for the tutorial demonstration, leading to a training with around 5 minutes. It may require much more training steps to converge.\n",
    "\n",
    "Please refer to [train_onlineRL.py](train_onlineRL.py) for the complete training codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
