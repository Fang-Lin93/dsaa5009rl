{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Begin to Develop Agents for Offline Reinforcement Learning\n",
    "\n",
    "This notebook aims to give a simple tutorial of the running logic of this offline RL project. Here is the outline:\n",
    "   1. Dataset: D4RL\n",
    "   2. Agent\n",
    "   3. Behavior Cloning Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Reinforcement Learning: a simplest review\n",
    "\n",
    "Let's think about playing a game, such as the game of playing the badminton:\n",
    "\n",
    "A **Reward** is the positive/negative feedback, such as win or loss.\n",
    "\n",
    "A **policy** takes observations (such as your opponents' position, your own position, your stamina ...) and produce **actions** (how to hit the shuttlecock, how to move) to maximize the expected reward.\n",
    "\n",
    "Reinforcement learning (RL) aims to learn a **policy** that can attains the maximal expected **reward**.\n",
    "\n",
    "Online RL aims to learn the **policy** by playing the badminton yourself. You iteratively do \"play -> win/loss -> summarize why you win/loss -> play...\"\n",
    "\n",
    "Offline RL aims to learn the **policy** by a **dataset** (such as watching the video from the world champion Lin Dan at home), then exercise the learned policy thereafter. You only do \"learn -> play\".\n",
    "\n",
    "In this project, we only consider offline RL, which aims to learn a data-driven decision maker. For a more comprehensive review, you can read this [paper](https://arxiv.org/abs/2005.01643)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. D4RL dataset\n",
    "\n",
    "[D4RL](https://github.com/digital-brain-sh/d4rl) provides standardized environments and datasets for training and benchmarking **offline RL** algorithms.\n",
    "\n",
    "D4RL can be installed by cloning the repository as follows:\n",
    "\n",
    "```\n",
    "git clone https://github.com/rail-berkeley/d4rl.git\n",
    "cd d4rl\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "The installation also installs [Mujoco](https://github.com/google-deepmind/mujoco) physics engine. If not, you need to install it for this project.\n",
    "Suppose we have download and installed the d4rl in your root repository, then we can load the dataset use the **gym** and **d4rl** package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:31:33.369409Z",
     "start_time": "2024-03-24T23:31:27.194130Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Aug 15 2022 11:36:51\n",
      "/Users/linjiajiefang/miniforge3/envs/offlineRL/lib/python3.9/site-packages/gym/spaces/box.py:84: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 10.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999998, 11)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import d4rl # Import required to register environments\n",
    "env_name = \"hopper-medium-v2\"\n",
    "# Create the environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Use d4rl.qlearning_dataset which adds next_observations.\n",
    "dataset = d4rl.qlearning_dataset(env)\n",
    "print(dataset['observations'].shape)  # Number of instances x Observation dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we have wrapped the dataset loading function for you in ```dataset.make_env_and_dataset```. This function loads the dataset and normalizes the rewards used in most research papers, and also create a wrapped ```env``` object to interactive with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:31:43.327834Z",
     "start_time": "2024-03-24T23:31:33.372288Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linjiajiefang/miniforge3/envs/offlineRL/lib/python3.9/site-packages/gym/spaces/box.py:84: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/Users/linjiajiefang/miniforge3/envs/offlineRL/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001B[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001B[0m\n",
      "  deprecation(\n",
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 10.66it/s]\n",
      "split to trajectories: 100%|██████████| 999998/999998 [00:01<00:00, 658216.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataset import make_env_and_dataset\n",
    "\n",
    "env_name = \"hopper-medium-v2\"\n",
    "\n",
    "env, dataset = make_env_and_dataset(env_name, seed = 520)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ignore the lowered precision warning and the deprecation warning. In this project, we only consider the  **\"hopper-medium-v2\"** dataset. The first time you load the data may take some time for downloading. You can sample a batch of dataset using ```dataset.sample(batch_size=batch_size)```. Here the **mask** information takes binary values: mask=0 if the game finishes and mask=1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:31:43.328415Z",
     "start_time": "2024-03-24T23:31:43.320883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transitions =  999998\n",
      "Batch size =  256\n",
      "observations shape =  (256, 11)\n",
      "actions shape =  (256, 3)\n",
      "rewards shape =  (256,)\n",
      "next_observations shape =  (256, 11)\n",
      "masks (1-done) shape =  (256,)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "sample_batch = dataset.sample(batch_size=batch_size)\n",
    "print(\"Number of transitions = \", dataset.size)\n",
    "print(\"Batch size = \", batch_size)\n",
    "print(\"observations shape = \", sample_batch.observations.shape)\n",
    "print(\"actions shape = \", sample_batch.actions.shape)\n",
    "print(\"rewards shape = \", sample_batch.rewards.shape)\n",
    "print(\"next_observations shape = \", sample_batch.next_observations.shape)\n",
    "print(\"masks (1-done) shape = \", sample_batch.masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agent\n",
    "\n",
    "An Agent object is the entity to learn from the dataset and also produce interactive actions. You need to write your own agent by inheriting from the ```Agent``` object in ```rlagents.agent```. All you need to do is to re-write the ```__init__```, ```update```, ```sample_actions``` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:31:43.329162Z",
     "start_time": "2024-03-24T23:31:43.325895Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset import Batch\n",
    "from typing import Dict, Any\n",
    "InfoDict = Dict[str, Any]\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    name = 'agent'\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # TODO: write your own way to initialize the agent: such as networks, optimizers, ...\n",
    "        pass\n",
    "\n",
    "    def update(self, batch: Batch) -> InfoDict:\n",
    "        # TODO: how to update the agent? the data batch contains five information: 'observations', 'actions', 'rewards', 'masks', 'next_observations'.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample_actions(self, observations: np.ndarray) -> np.ndarray:\n",
    "        # TODO: how to produce actions for environment interation. A typical action is a np.array vector ranges in [-1, 1].\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a random agent without the requirement of training can be defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:31:43.337477Z",
     "start_time": "2024-03-24T23:31:43.330494Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    name = 'random agent without training'\n",
    "\n",
    "    def __init__(self, action_space: gym.spaces.box.Box):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def update(self, batch) -> InfoDict:\n",
    "        pass\n",
    "\n",
    "    def sample_actions(self, observations):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "rand_agent = RandomAgent(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can evaluate the agent by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:31:43.383340Z",
     "start_time": "2024-03-24T23:31:43.334065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 1.3120474228487389, 'median': 1.2027454683943803, 'std': 0.4506554193728289, 'min': 0.8491734081854676, 'max': 2.4467042480069217, 'length': 26.7}\n"
     ]
    }
   ],
   "source": [
    "from eval import evaluate\n",
    "res = evaluate(rand_agent, env=env, num_episodes=10, render=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Behavior Cloning Example\n",
    "\n",
    "To make the coding simpler, we give behavior-cloning (BC) agents as examples. BC agents take observations as inputs, and predict the actions in the dataset. In our ```hopper-medium-v2``` example, the input observation is of dimension 11, the output action is of dimension 3.\n",
    "\n",
    "We provide two versions of agent design for you to refer to: coded with [Pytorch](https://pytorch.org/get-started/locally/) or [jax](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)&[flax](https://flax.readthedocs.io/en/latest/getting_started.html). The jax agent is slightly faster than the torch agent, while the torch agent is more human-readable. You can also use tensorflow if you like. There is no constrain for the usage of deep learning packages. But do not use well-coded learner from other people. Try to write your own training/testing process.\n",
    "\n",
    "\n",
    "Here we use the torch agent as the example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports... \n",
    "You may need to install some packages if there is any package not found error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:31:44.834753Z",
     "start_time": "2024-03-24T23:31:43.373286Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "from tensorboardX import SummaryWriter\n",
    "from dataset import make_env_and_dataset\n",
    "from tqdm import trange\n",
    "from agents import TorchBCLearner, JAXBCLearner\n",
    "from eval import eval_agent, STATISTICS\n",
    "from utils import prepare_output_dir, set_torch_seed\n",
    "# for tensorboard visualization in jupyter notebook only\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation for the results recording: set seeds, create save folder and summary writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:31:44.844213Z",
     "start_time": "2024-03-24T23:31:44.836392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are saved in 'results/20240325-073144_Behavior-Cloning' \n"
     ]
    }
   ],
   "source": [
    "# set the seed\n",
    "seed = 520\n",
    "set_torch_seed(seed)\n",
    "\n",
    "# create a saving directory\n",
    "save_dir = prepare_output_dir(suffix=\"Behavior-Cloning\")\n",
    "with open(os.path.join(save_dir, f\"seed_{seed}.txt\"), \"w\") as f:\n",
    "    print(\"\\t\".join([\"steps\"] + STATISTICS), file=f)\n",
    "summary_writer = SummaryWriter(os.path.join(save_dir, 'tensorboard', f'seed={seed}'))\n",
    "print(f\"Results are saved in '{save_dir}' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters: here we model the actions using MLP with 3 hidden layers, each of dim=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:31:44.844741Z",
     "start_time": "2024-03-24T23:31:44.841921Z"
    }
   },
   "outputs": [],
   "source": [
    "max_steps = 100000  # maximal number of training steps, 100000 is for this tutorial only and it's too short for most methods. You may need to try 1M~2M\n",
    "eval_interval = 5000  # evaluate the agent every 'eval_interval' gradient steps\n",
    "log_interval = 1000  # record the training statistics, such as loss every 'log_interval' gradient steps\n",
    "num_eval_episodes = 10  # number of evaluation episodes for each evaluation. Should be >= 10 for stability\n",
    "batch_size = 256\n",
    "hidden_dims = (256, 256, 256)  # for MLP with 3 hidden layers, each of dim=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch dataset and create corresponding env:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:31:54.778918Z",
     "start_time": "2024-03-24T23:31:44.844562Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linjiajiefang/miniforge3/envs/offlineRL/lib/python3.9/site-packages/gym/spaces/box.py:84: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/Users/linjiajiefang/miniforge3/envs/offlineRL/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001B[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001B[0m\n",
      "  deprecation(\n",
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 10.76it/s]\n",
      "split to trajectories: 100%|██████████| 999998/999998 [00:01<00:00, 661684.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# fetch dataset and the corresponding environment\n",
    "env_name = 'hopper-medium-v2'\n",
    "env, dataset = make_env_and_dataset(env_name, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the torch agent, you can use ```torch.device('cuda: 0')``` if cuda is available. For jax-coded agent, you can use \n",
    "\n",
    "```\n",
    "agent = JAXBCLearner(seed=seed,\n",
    "                     obs_dim=obs_dim,\n",
    "                     act_dim=act_dim,\n",
    "                     actor_lr=3e-4,\n",
    "                     layer_norm=True,\n",
    "                     hidden_dims=hidden_dims,\n",
    "                     lr_decay_T=max_steps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:31:54.954561Z",
     "start_time": "2024-03-24T23:31:54.781986Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_dim = len(env.observation_space.sample())\n",
    "act_dim = len(env.action_space.sample())\n",
    "agent = TorchBCLearner(obs_dim=obs_dim,\n",
    "                       act_dim=act_dim,\n",
    "                       actor_lr=3e-4,\n",
    "                       layer_norm=True,\n",
    "                       hidden_dims=hidden_dims,\n",
    "                       lr_decay_T=max_steps,\n",
    "                       device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the main training and evaluation process. Record the latest 5 mean returns as the final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:38:16.532418Z",
     "start_time": "2024-03-24T23:31:54.954848Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 42/100000 [00:00<07:51, 211.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=0, Eval Mean=0.868019780407904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4994/100000 [00:21<06:45, 234.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=5000, Eval Mean=45.02583104081061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10043/100000 [00:43<16:53, 88.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=10000, Eval Mean=48.937190052511085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15030/100000 [01:06<14:31, 97.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=15000, Eval Mean=46.7221872956148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20030/100000 [01:28<16:53, 78.87it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=20000, Eval Mean=55.363668854270884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25049/100000 [01:50<14:23, 86.84it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=25000, Eval Mean=47.96478300354196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30048/100000 [02:12<12:23, 94.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=30000, Eval Mean=39.40810451253631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35038/100000 [02:31<09:50, 110.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=35000, Eval Mean=48.99787741237678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40054/100000 [02:49<07:57, 125.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=40000, Eval Mean=42.49380795069705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45044/100000 [03:07<07:17, 125.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=45000, Eval Mean=41.77125900212759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50050/100000 [03:24<06:31, 127.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=50000, Eval Mean=45.46947967646627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55039/100000 [03:42<06:13, 120.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=55000, Eval Mean=48.07477505858807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60033/100000 [04:00<05:12, 127.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=60000, Eval Mean=42.96874110515497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65029/100000 [04:17<05:40, 102.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=65000, Eval Mean=44.6270015130715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70044/100000 [04:35<04:27, 111.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=70000, Eval Mean=48.215093696345875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75035/100000 [04:53<03:56, 105.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=75000, Eval Mean=50.62383224522444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80046/100000 [05:11<02:43, 122.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=80000, Eval Mean=44.46050057606199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85032/100000 [05:29<01:59, 125.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=85000, Eval Mean=45.93610000788423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90037/100000 [05:46<01:18, 127.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=90000, Eval Mean=45.80896832047246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95048/100000 [06:04<00:41, 119.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=95000, Eval Mean=46.78829250340031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [06:21<00:00, 262.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Mean Return=46.72353873060868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "latest_mean_returns = deque(maxlen=5)  # track the performance of the latest 5 evaluation\n",
    "for i in trange(max_steps):\n",
    "\n",
    "    # evaluation\n",
    "    if i % eval_interval == 0:\n",
    "        eval_res = eval_agent(i, agent, env, summary_writer, save_dir, seed, num_eval_episodes)\n",
    "        latest_mean_returns.append(eval_res['mean'])\n",
    "        print(f\"Step={i}, Eval Mean={eval_res['mean']}\")\n",
    "\n",
    "    # training process\n",
    "    batch = dataset.sample(batch_size)\n",
    "    update_info = agent.update(batch)\n",
    "\n",
    "    # record the training information\n",
    "    if i % log_interval == 0:\n",
    "        for k, v in update_info.items():\n",
    "            summary_writer.add_scalar(f'training/{k}', v, i)\n",
    "        summary_writer.flush()\n",
    "\n",
    "print(f\"Final Mean Return={np.mean(latest_mean_returns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After few minutes training, you should get a BC agent with final mean return around 50. To view the training process, you can use ```tensorboard``` to track the training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:38:20.180307Z",
     "start_time": "2024-03-24T23:38:16.532805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {save_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the video of how the agent control the hopper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:38:25.118196Z",
     "start_time": "2024-03-24T23:38:20.183211Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linjiajiefang/miniforge3/envs/offlineRL/lib/python3.9/site-packages/gym/envs/registration.py:505: UserWarning: \u001B[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v3` with the environment ID `Hopper-v3`.\u001B[0m\n",
      "  logger.warn(\n",
      "/Users/linjiajiefang/miniforge3/envs/offlineRL/lib/python3.9/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001B[33mWARN: Overwriting existing videos at /Users/linjiajiefang/PycharmProjects/dsaa5009rl/results/20240325-073144_Behavior-Cloning folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001B[0m\n",
      "  logger.warn(\n",
      "/Users/linjiajiefang/miniforge3/envs/offlineRL/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001B[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001B[0m\n",
      "  logger.deprecation(\n",
      "/Users/linjiajiefang/miniforge3/envs/offlineRL/lib/python3.9/subprocess.py:1754: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n",
      "The video is saved in results/20240325-073144_Behavior-Cloning as a '.mp4' file!\n"
     ]
    }
   ],
   "source": [
    "# create the video\n",
    "env = gym.make(\"Hopper-v2\")\n",
    "env = gym.wrappers.RecordVideo(env, save_dir)\n",
    "observation, done = env.reset(), False\n",
    "while not done:\n",
    "    action = agent.sample_actions(observation)  # eval takes argmax from actor net\n",
    "    observation, _, done, info = env.step(np.clip(action, -1, 1))\n",
    "env.close()\n",
    "\n",
    "print(f\"The video is saved in {save_dir} as a '.mp4' file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A compact code version can be found in ```main.py``` for references. You can directly run\n",
    "\n",
    "```\n",
    "python main.py --agent torchBC --create_video\n",
    "```\n",
    "\n",
    "to produce the whole train/test results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:38:25.119718Z",
     "start_time": "2024-03-24T23:38:25.118460Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
